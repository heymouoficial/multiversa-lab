# Spec: Arquitectura de Memoria RAG (Portality Brain)

**Autor:** Arquitecto de Soluciones AI (Multiversa Lab)
**Estado:** Draft
**Objetivo:** Implementar capacidad de memoria a largo plazo y recuperación de contexto (RAG) para el agente Savara, gestionado desde Portality.

## 1. Arquitectura de Datos (Supabase PostgreSQL)

Utilizaremos `pgvector` para almacenamiento vectorial. Dado el stack Gemini, usaremos el modelo `gemini-embedding-001` (o `text-embedding-004`), estandarizando la dimensión a **768** (usando Matryoshka learning para eficiencia y menor latencia, soportado por Gemini).

### 1.1 Extensión y Tabla `knowledge_base`

```sql
-- Habilitar extensión pgvector
create extension if not exists vector with schema extensions;

-- Tabla principal de conocimiento
create table public.knowledge_base (
  id bigint generated by default as identity primary key,
  content text not null, -- El chunk de texto
  embedding extensions.vector(768), -- Vector de 768 dimensiones (Gemini Optimized)
  metadata jsonb default '{}'::jsonb, -- Ej: { "filename": "manual.pdf", "type": "pdf", "page": 1 }
  machine_id text default 'global', -- 'global' para todos, o UUID específico de la máquina/licencia
  created_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- Habilitar RLS (Row Level Security)
alter table public.knowledge_base enable row level security;

-- Política de lectura (ajustar según auth de Portality)
create policy "Allow read access" on public.knowledge_base for select using (true);
```

### 1.2 Índices (HNSW)
Para producción y escalabilidad, preferimos índices HNSW sobre IVFFlat por su mejor rendimiento en búsquedas aproximadas y capacidad de construirse incrementalmente.

```sql
create index on public.knowledge_base using hnsw (embedding extensions.vector_cosine_ops);
-- Opcional: Índice en metadata para filtrado rápido
create index idx_knowledge_metadata on public.knowledge_base using gin (metadata);
create index idx_knowledge_machine_id on public.knowledge_base (machine_id);
```

### 1.3 Función de Búsqueda (RPC)
Función optimizada para evitar el problema de "overfiltering" mediante filtrado de metadatos.

```sql
create or replace function match_documents (
  query_embedding extensions.vector(768),
  match_threshold float,
  match_count int,
  filter_machine_id text default 'global'
)
returns table (
  id bigint,
  content text,
  similarity float,
  metadata jsonb
)
language plpgsql
as $$
begin
  return query
  select
    kb.id,
    kb.content,
    1 - (kb.embedding <=> query_embedding) as similarity,
    kb.metadata
  from public.knowledge_base kb
  where 1 - (kb.embedding <=> query_embedding) > match_threshold
  -- Lógica de filtro: Trae documentos globales O específicos de la máquina
  and (kb.machine_id = 'global' or kb.machine_id = filter_machine_id)
  order by kb.embedding <=> query_embedding
  limit match_count;
end;
$$;
```

---

## 2. Infraestructura Backend (Supabase Edge Functions)

Dado que estamos en un entorno Edge (Deno), no podemos usar librerías de Node.js que dependan de binarios nativos pesados. Usaremos librerías nativas de Deno o JSR.

### 2.1 Función `ingest-file`
*   **Runtime:** Deno (Supabase Edge Runtime).
*   **Parser PDF:** Usaremos `@pdf/pdftext` (vía JSR) por ser ligero y compatible con Edge.
*   **AI Provider:** Google Generative AI (Gemini) para embeddings.
*   **Flujo:**
    1.  Recibe `FormData` (archivo + `machine_id`).
    2.  Valida tipo MIME (PDF/TXT/MD).
    3.  Extrae texto usando `@pdf/pdftext` (si es PDF).
    4.  Chunking: Divide el texto en segmentos (ej. 1000 caracteres con overlap de 200).
    5.  Genera embeddings batch con `gemini-embedding-001`.
    6.  Inserta en Supabase `knowledge_base`.

**Snippet Conceptual (Deno):**
```typescript
import { pdfText } from 'jsr:@pdf/pdftext'; //
import { createClient } from 'jsr:@supabase/supabase-js@2';
import { GoogleGenerativeAI } from 'npm:@google/generative-ai';

// ... configuración de clientes ...

// Lógica de embedding
const model = genAI.getGenerativeModel({ model: "text-embedding-004" });
const result = await model.embedContent(textChunk);
const vector = result.embedding.values; // Array de floats
// Guardar en DB...
```

---


## 3. Frontend (Portality UI)

### 3.1 Componente `DropZone`
*   Área interactiva para arrastrar archivos.
*   Selector booleano: "¿Conocimiento Global o Específico de esta Licencia?".
*   Visualización de progreso de carga e ingestión (Spinner mientras la Edge Function procesa).

### 3.2 Componente `MemoryTable`
*   Tabla listando entradas de la tabla `knowledge_base`.
*   Columnas: Icono (PDF/TXT), Nombre de archivo (metadata->filename), Machine ID, Fecha, Acción (Borrar).
*   **Feature "Olvido":** Botón para eliminar registros por ID, permitiendo corregir alucinaciones causadas por mala data.

### 3.3 Vinculación de Identidad
*   En el módulo "Forjado de Licencias", añadir campo `user_name`.
*   Guardar en `metadata` de la licencia. Savara usará esto para personalizar el saludo.

---

## 4. Integración Chat (Inferencia)

### 4.1 Modificación de `useSavaraLive` / `geminiService`
El hook de chat debe evolucionar de una llamada directa a una orquestada:

1.  **Interceptar Prompt:** Antes de enviar a Gemini Flash.
2.  **Recuperación (Retrieval):** Llamar a `rpc/match_documents` con el vector del prompt del usuario y el `machine_id` actual.
3.  **Construcción de Contexto:**
    Si hay resultados con similitud > 0.7:
    ```javascript
    const contextBlock = matches.map(m => m.content).join("\n---\n");
    const systemInstructionWithRAG = `
      ${originalSystemPrompt}
      
      INFORMACIÓN DE CONTEXTO RECUPERADA (Priorizar sobre conocimiento general):
      ${contextBlock}
    `;
    ```
4.  **Generación:** Enviar el `systemInstruction` enriquecido a Gemini Flash.

```